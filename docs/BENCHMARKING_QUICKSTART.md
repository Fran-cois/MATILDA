# ğŸš€ MATILDA Benchmarking - Quick Start

## ğŸ¯ Pour les PressÃ©s

**Vous voulez benchmarker MATILDA pour une publication ?**

```bash
# ONE COMMAND = Tout automatique â­
python run_full_benchmark.py --runs 5
```

âœ… ExÃ©cute tous les algorithmes  
âœ… Calcule statistiques (moyenne Â± std)  
âœ… GÃ©nÃ¨re table LaTeX professionnelle  
âœ… Sauvegarde rÃ©sultats JSON  

**DurÃ©e estimÃ©e :** 1-2 heures  
**Output :** `data/output/benchmark_table_*.tex`

---

## ğŸ“Š Trois Scripts, Trois Usages

### 1ï¸âƒ£ `run_full_benchmark.py` - Benchmark Complet ğŸ“

**Quand :** Article scientifique, comparaison complÃ¨te

```bash
# Benchmark TOUT : tous algorithmes Ã— tous datasets Ã— N runs
python run_full_benchmark.py --runs 5

# CiblÃ© : 2 algorithmes spÃ©cifiques
python run_full_benchmark.py --runs 5 --algorithms MATILDA SPIDER

# Avec config
python run_full_benchmark.py --config benchmark_config.yaml
```

**Output :**
- âœ… RÃ©sultats JSON avec toutes les exÃ©cutions
- âœ… Statistiques JSON (mean, std, n_runs)
- âœ… Table LaTeX avec format `$9 \pm 0.0$`
- âœ… RÃ©sumÃ© console

**Temps :** 1-4h selon nombre de runs  
**Doc :** [FULL_BENCHMARK_GUIDE.md](FULL_BENCHMARK_GUIDE.md)

---

### 2ï¸âƒ£ `run_benchmark.py` - Benchmark CiblÃ© ğŸ“ˆ

**Quand :** Test d'un algorithme spÃ©cifique avec stats

```bash
# Benchmarker MATILDA seulement (5 runs)
python run_benchmark.py --runs 5 --algorithms MATILDA

# Sur datasets spÃ©cifiques
python run_benchmark.py --runs 5 --datasets Bupa BupaImperfect
```

**Output :**
- âœ… RÃ©sultats JSON d'un algorithme
- âœ… Statistiques pour cet algorithme
- âœ… Table LaTeX partielle

**Temps :** 5-30 min selon algorithme  
**Doc :** [LATEX_TABLES_GUIDE.md](LATEX_TABLES_GUIDE.md)

---

### 3ï¸âƒ£ `generate_latex_table.py` - Table Rapide âš¡

**Quand :** RÃ©sultats dÃ©jÃ  calculÃ©s, besoin d'une table vite

```bash
# Table dÃ©taillÃ©e depuis rÃ©sultats existants (< 1 seconde)
python generate_latex_table.py --detailed

# Table simple (6 colonnes)
python generate_latex_table.py

# Algorithmes spÃ©cifiques
python generate_latex_table.py --algorithms MATILDA SPIDER
```

**Output :**
- âœ… Table LaTeX depuis fichiers `*_results.json` existants
- âŒ Pas de statistiques (valeurs uniques)

**Temps :** < 1 seconde  
**Doc :** [LATEX_README.md](LATEX_README.md)

---

## ğŸ¤” Quel Script Choisir ?

| Besoin | Script | Commande |
|--------|--------|----------|
| ğŸ“„ **Article complet** | `run_full_benchmark.py` | `python run_full_benchmark.py --runs 5` |
| ğŸ§ª **Tester 1 algo** | `run_benchmark.py` | `python run_benchmark.py --runs 5 --algorithms MATILDA` |
| âš¡ **Table immÃ©diate** | `generate_latex_table.py` | `python generate_latex_table.py --detailed` |

**Guide dÃ©taillÃ© :** [WHICH_SCRIPT.md](WHICH_SCRIPT.md)

---

## ğŸ“ Fichiers de Configuration

### `benchmark_config.yaml`

```yaml
# Nombre de runs par combinaison algo/dataset
runs: 5

# Algorithmes Ã  benchmarker
algorithms:
  - MATILDA
  - SPIDER
  - ANYBURL
  - POPPER

# Datasets
datasets:
  - Bupa
  - BupaImperfect
  - ComparisonDataset
  - ImperfectTest

# Options
table_type: detailed  # ou 'simple'
timeout: 3600         # 1 heure par run
```

**Usage :**
```bash
python run_full_benchmark.py --config benchmark_config.yaml
```

---

## ğŸ“Š Formats de Sortie

### Table Simple (6 colonnes)

```latex
\textbf{Algorithm} & \textbf{Dataset} & \textbf{\#Rules} & 
\textbf{Accuracy} & \textbf{Confidence} & \textbf{Time (s)}
```

### Table DÃ©taillÃ©e (8 colonnes)

```latex
\textbf{Algorithm} & \textbf{Dataset} & \textbf{\#Rules} & 
\textbf{Acc.} & \textbf{Conf.} & \textbf{T_compat} & 
\textbf{T_index} & \textbf{T_CG}
```

### Format Statistiques

Avec N runs :
```latex
MATILDA & Bupa & $9 \pm 0.0$ & $1.000 \pm 0.000$ & ...
```

---

## ğŸ§ª Tester

```bash
# VÃ©rifier que tout fonctionne
python test_latex_generation.py

# Test rapide (1 run, 1 algo)
python run_full_benchmark.py --runs 1 --algorithms MATILDA --datasets Bupa
```

---

## â±ï¸ Temps d'ExÃ©cution

| Configuration | Temps EstimÃ© |
|---------------|--------------|
| `generate_latex_table.py` | < 1 seconde |
| `run_benchmark.py --runs 5` (1 algo) | 5-15 minutes |
| `run_full_benchmark.py --runs 3` | 30-60 minutes |
| `run_full_benchmark.py --runs 5` | 1-2 heures |
| `run_full_benchmark.py --runs 10` | 3-4 heures |

---

## ğŸ“š Documentation ComplÃ¨te

| Document | Contenu |
|----------|---------|
| [FULL_BENCHMARK_GUIDE.md](FULL_BENCHMARK_GUIDE.md) | Guide `run_full_benchmark.py` |
| [LATEX_TABLES_GUIDE.md](LATEX_TABLES_GUIDE.md) | Guide `run_benchmark.py` |
| [LATEX_README.md](LATEX_README.md) | Guide `generate_latex_table.py` |
| [WHICH_SCRIPT.md](WHICH_SCRIPT.md) | Arbre de dÃ©cision |
| [LATEX_SUMMARY.md](LATEX_SUMMARY.md) | RÃ©fÃ©rence ultra-concise |
| [benchmark_config.yaml](benchmark_config.yaml) | Config exemple |

---

## ğŸ¯ Workflow RecommandÃ©

### Pour Article Scientifique

```bash
# 1. Test rapide (vÃ©rifier)
python run_full_benchmark.py --runs 1 --algorithms MATILDA --datasets Bupa

# 2. Benchmark complet (1-2h)
python run_full_benchmark.py --runs 5

# 3. Copier table dans article
cp data/output/benchmark_table_*.tex paper/tables/

# 4. Compiler LaTeX
pdflatex paper/main.tex
```

### Pour PrÃ©sentation

```bash
# Table rapide depuis rÃ©sultats existants
python generate_latex_table.py --detailed

# Ou benchmark rapide si nouveaux rÃ©sultats
python run_full_benchmark.py --runs 3 --algorithms MATILDA SPIDER
```

---

## ğŸ’¡ Tips

### RÃ©duire le Temps d'ExÃ©cution

```bash
# Moins de runs
python run_full_benchmark.py --runs 3

# Moins d'algorithmes
python run_full_benchmark.py --runs 5 --algorithms MATILDA SPIDER

# Moins de datasets
python run_full_benchmark.py --runs 5 --datasets Bupa BupaImperfect
```

### GÃ©rer les Timeouts

```bash
# Augmenter timeout (2 heures)
python run_full_benchmark.py --runs 5 --timeout 7200
```

### ExÃ©cuter en ArriÃ¨re-Plan

```bash
# Lancer et continuer Ã  travailler
nohup python run_full_benchmark.py --runs 5 > benchmark.log 2>&1 &

# Suivre progression
tail -f benchmark.log
```

---

## ğŸ†˜ Aide

```bash
# Aide gÃ©nÃ©rale
python run_full_benchmark.py --help
python run_benchmark.py --help
python generate_latex_table.py --help

# Tests
python test_latex_generation.py
```

---

## âœ… Checklist Article

- [ ] ExÃ©cuter `python run_full_benchmark.py --runs 5`
- [ ] VÃ©rifier `data/output/benchmark_table_*.tex`
- [ ] VÃ©rifier statistiques : Ã©cart-type < 10% de moyenne
- [ ] Copier table dans `paper/tables/`
- [ ] Compiler LaTeX et vÃ©rifier rendu
- [ ] Documenter mÃ©thodologie (5 runs, timeout 1h, etc.)
- [ ] Sauvegarder `full_benchmark_results_*.json` et `*_statistics_*.json`

---

## ğŸ‰ C'est Tout !

**Pour la plupart des cas :**
```bash
python run_full_benchmark.py --runs 5
```

**Besoin d'aide ?** Consultez [WHICH_SCRIPT.md](WHICH_SCRIPT.md) ou [FULL_BENCHMARK_GUIDE.md](FULL_BENCHMARK_GUIDE.md)
