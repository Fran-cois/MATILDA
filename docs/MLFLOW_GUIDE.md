# ğŸ“Š MLflow-like Experiment Tracking Guide

## Vue d'ensemble

Le systÃ¨me de benchmarking MATILDA suit maintenant une architecture MLflow avec :

- **ExpÃ©riences** : Regroupent plusieurs runs (exÃ©cutions)
- **Runs** : ExÃ©cutions individuelles avec params, mÃ©triques et artefacts
- **MÃ©triques** : Valeurs numÃ©riques trackÃ©es (accuracy, time, etc.)
- **ParamÃ¨tres** : Configuration (algorithm, dataset, timeout)
- **Artefacts** : Fichiers gÃ©nÃ©rÃ©s (tables LaTeX, rÃ¨gles JSON)

---

## ğŸš€ Quick Start

### Lancer un Benchmark

```bash
# Benchmark avec nom d'expÃ©rience personnalisÃ©
python3 run_full_benchmark.py --runs 5 --experiment-name "comparison_all_algos"

# Benchmark avec configuration par dÃ©faut
python3 run_full_benchmark.py --runs 5
```

### Explorer les RÃ©sultats

```bash
# Lister toutes les expÃ©riences
python3 mlflow_explorer.py list

# Voir les dÃ©tails d'une expÃ©rience
python3 mlflow_explorer.py show <experiment_id>

# Lister les runs d'une expÃ©rience
python3 mlflow_explorer.py runs <experiment_id>

# Comparer deux expÃ©riences
python3 mlflow_explorer.py compare <exp_id1> <exp_id2>
```

---

## ğŸ“ Structure des Fichiers

### Organisation MLflow

```
data/output/mlruns/
â”œâ”€â”€ <experiment_id>/                    # Un rÃ©pertoire par expÃ©rience
â”‚   â”œâ”€â”€ experiment_meta.json           # MÃ©tadonnÃ©es de l'expÃ©rience
â”‚   â”œâ”€â”€ runs.json                      # Tous les runs de l'expÃ©rience
â”‚   â”œâ”€â”€ summary.json                   # Statistiques agrÃ©gÃ©es
â”‚   â”œâ”€â”€ benchmark_table_*.tex          # Table LaTeX gÃ©nÃ©rÃ©e
â”‚   â”‚
â”‚   â””â”€â”€ <run_id>/                      # Un rÃ©pertoire par run
â”‚       â”œâ”€â”€ run_info.json              # Info du run (status, times)
â”‚       â”œâ”€â”€ params.json                # ParamÃ¨tres du run
â”‚       â”œâ”€â”€ metrics.json               # MÃ©triques du run
â”‚       â””â”€â”€ rules.json                 # RÃ¨gles dÃ©couvertes
```

### Fichiers Principaux

#### `experiment_meta.json`
```json
{
  "experiment_id": "f1f769ba",
  "name": "comparison_all_algos",
  "artifact_location": "data/output/mlruns/f1f769ba",
  "lifecycle_stage": "active",
  "creation_time": "20260112_161312",
  "tags": {
    "num_algorithms": 4,
    "num_datasets": 4,
    "runs_per_combination": 5
  }
}
```

#### `run_info.json`
```json
{
  "run_id": "2c74edc2-a90f-49",
  "run_name": "MATILDA_Bupa_run1",
  "experiment_id": "f1f769ba",
  "status": "FINISHED",  // ou "FAILED", "RUNNING"
  "start_time": "2026-01-12T16:13:10.378613",
  "end_time": "2026-01-12T16:13:12.565551",
  "artifact_uri": "data/output/mlruns/f1f769ba/2c74edc2-a90f-49"
}
```

#### `params.json`
```json
{
  "algorithm": "MATILDA",
  "dataset": "Bupa",
  "run_number": 1,
  "timeout": 3600
}
```

#### `metrics.json`
```json
{
  "num_rules": 9,
  "accuracy": 1.0,
  "confidence": 1.0,
  "time_total": 0.124,
  "time_compat": 0.038,
  "time_index": 0.038,
  "time_cg": 0.048,
  "duration_seconds": 2.19
}
```

#### `summary.json`
```json
{
  "MATILDA_Bupa": {
    "algorithm": "MATILDA",
    "dataset": "Bupa",
    "runs": [
      { "run_id": "xxx", "metrics": {...} },
      ...
    ],
    "metrics": {
      "num_rules": {
        "mean": 9.0,
        "std": 0.0,
        "min": 9,
        "max": 9,
        "count": 5
      },
      "accuracy": { "mean": 1.0, "std": 0.0, ... },
      ...
    }
  }
}
```

---

## ğŸ” Utilisation de l'Explorateur

### Commandes Disponibles

#### 1. Lister les ExpÃ©riences

```bash
python3 mlflow_explorer.py list
```

**Output:**
```
================================================================================
ID           Name                           Created              Status    
================================================================================
f1f769ba     test_mlflow_integration        20260112_161312      active    
a3b5c789     comparison_all_algos           20260112_143020      active    
================================================================================
Total experiments: 2
```

#### 2. DÃ©tails d'une ExpÃ©rience

```bash
python3 mlflow_explorer.py show f1f769ba
```

**Output:**
```
EXPERIMENT: test_mlflow_integration
ID:           f1f769ba
Location:     data/output/mlruns/f1f769ba
Status:       active

CONFIGURATION
  num_algorithms: 1
  num_datasets: 1
  runs_per_combination: 5

RUNS (5 total)
  âœ“ Finished: 5
  âœ— Failed:   0

SUMMARY STATISTICS
MATILDA on Bupa:
  Runs: 5
  Rules:      9.0 Â± 0.0 (min=9, max=9)
  Accuracy:   1.000 Â± 0.000
  Duration:   2.19s Â± 0.12s
```

#### 3. Lister les Runs

```bash
# Tous les runs
python3 mlflow_explorer.py runs f1f769ba

# Seulement les runs terminÃ©s
python3 mlflow_explorer.py runs f1f769ba --status FINISHED

# Seulement les runs Ã©chouÃ©s
python3 mlflow_explorer.py runs f1f769ba --status FAILED
```

#### 4. DÃ©tails d'un Run SpÃ©cifique

```bash
python3 mlflow_explorer.py run f1f769ba 2c74edc2
```

#### 5. Comparer Deux ExpÃ©riences

```bash
python3 mlflow_explorer.py compare f1f769ba a3b5c789
```

**Output:**
```
COMPARISON: f1f769ba vs a3b5c789

MATILDA on Bupa:
Metric               Exp1                      Exp2                      Diff           
-----------------------------------------------------------------------------------
num_rules            9.000 Â± 0.000             9.000 Â± 0.000             +0.000 (+0.0%) 
accuracy             1.000 Â± 0.000             0.998 Â± 0.002             -0.002 (-0.2%) 
duration_seconds     2.190 Â± 0.120             2.345 Â± 0.098             +0.155 (+7.1%) 
```

---

## ğŸ“Š Workflows Typiques

### 1. Benchmark et Analyse

```bash
# 1. Lancer benchmark
python3 run_full_benchmark.py --runs 5 --experiment-name "baseline_v1"

# 2. Noter l'experiment_id (affichÃ© dans l'output)
# Exemple: f1f769ba

# 3. Explorer les rÃ©sultats
python3 mlflow_explorer.py show f1f769ba

# 4. VÃ©rifier les runs
python3 mlflow_explorer.py runs f1f769ba
```

### 2. Comparer Deux Versions

```bash
# 1. Benchmark version 1
python3 run_full_benchmark.py --runs 5 --experiment-name "v1_baseline"
# -> experiment_id: abc123

# 2. Modifier l'algorithme, puis benchmark version 2
python3 run_full_benchmark.py --runs 5 --experiment-name "v2_improved"
# -> experiment_id: def456

# 3. Comparer
python3 mlflow_explorer.py compare abc123 def456
```

### 3. Analyser des Ã‰checs

```bash
# 1. Lister les runs Ã©chouÃ©s
python3 mlflow_explorer.py runs f1f769ba --status FAILED

# 2. Examiner un run Ã©chouÃ©
python3 mlflow_explorer.py run f1f769ba <failed_run_id>

# 3. VÃ©rifier les logs ou artifacts
cat data/output/mlruns/f1f769ba/<run_id>/run_info.json
```

---

## ğŸ¯ Cas d'Usage

### Article Scientifique

```bash
# Benchmark pour publication
python3 run_full_benchmark.py --runs 10 --experiment-name "paper_final_results"

# VÃ©rifier statistiques
python3 mlflow_explorer.py show <exp_id>

# Table LaTeX gÃ©nÃ©rÃ©e automatiquement dans:
# data/output/mlruns/<exp_id>/benchmark_table_*.tex
```

### Tests de RÃ©gression

```bash
# Benchmark avant modification
python3 run_full_benchmark.py --runs 5 --experiment-name "before_refactor"

# Faire les modifications du code...

# Benchmark aprÃ¨s modification
python3 run_full_benchmark.py --runs 5 --experiment-name "after_refactor"

# Comparer
python3 mlflow_explorer.py compare <before_id> <after_id>
```

### Optimisation de ParamÃ¨tres

```bash
# Test avec DFS
# (modifier config: traversal_algorithm: dfs)
python3 run_full_benchmark.py --runs 5 --experiment-name "matilda_dfs"

# Test avec BFS
# (modifier config: traversal_algorithm: bfs)
python3 run_full_benchmark.py --runs 5 --experiment-name "matilda_bfs"

# Test avec A*
# (modifier config: traversal_algorithm: astar)
python3 run_full_benchmark.py --runs 5 --experiment-name "matilda_astar"

# Comparer tous
python3 mlflow_explorer.py compare <dfs_id> <bfs_id>
python3 mlflow_explorer.py compare <bfs_id> <astar_id>
```

---

## ğŸ’¾ CompatibilitÃ©

### Format Legacy

Pour compatibilitÃ© avec les anciens scripts, les rÃ©sultats sont aussi sauvegardÃ©s dans le format legacy :

```
data/output/
â”œâ”€â”€ full_benchmark_results_*.json      # Format ancien
â”œâ”€â”€ full_benchmark_statistics_*.json   # Format ancien
â””â”€â”€ benchmark_table_*.tex              # Table LaTeX (copie)
```

Ces fichiers peuvent Ãªtre utilisÃ©s avec les anciens scripts comme `generate_latex_table.py`.

---

## ğŸ”§ IntÃ©gration avec MLflow Officiel

Si vous voulez utiliser le vrai MLflow UI :

### 1. Installer MLflow

```bash
pip install mlflow
```

### 2. Convertir les DonnÃ©es

Les donnÃ©es sont dÃ©jÃ  dans un format compatible. Vous pouvez :

```bash
# Lancer l'UI MLflow
cd data/output
mlflow ui --backend-store-uri mlruns/
```

### 3. Voir dans le Navigateur

Ouvrez http://localhost:5000 pour explorer visuellement vos expÃ©riences.

---

## ğŸ“ˆ MÃ©triques TrackÃ©es

| MÃ©trique | Description | UnitÃ© |
|----------|-------------|-------|
| `num_rules` | Nombre de rÃ¨gles dÃ©couvertes | count |
| `accuracy` | PrÃ©cision des rÃ¨gles | 0-1 |
| `confidence` | Confiance des rÃ¨gles | 0-1 |
| `time_total` | Temps total | seconds |
| `time_compat` | Temps compatibility graph | seconds |
| `time_index` | Temps indexation | seconds |
| `time_cg` | Temps construction CG | seconds |
| `duration_seconds` | DurÃ©e complÃ¨te du run | seconds |

---

## ğŸ“ Best Practices

### 1. Nommage des ExpÃ©riences

```bash
# âœ… Bon : Descriptif et datÃ©
--experiment-name "comparison_algos_2026_01_12"
--experiment-name "matilda_bfs_optimization_v2"

# âŒ Mauvais : Trop vague
--experiment-name "test"
--experiment-name "exp1"
```

### 2. Nombre de Runs

- **Test rapide** : 1-3 runs
- **DÃ©veloppement** : 3-5 runs
- **Publication** : 5-10 runs
- **Haute prÃ©cision** : 10+ runs

### 3. Organisation

```bash
# Garder les expÃ©riences organisÃ©es par projet
--experiment-name "project_task_version"

# Exemples:
--experiment-name "paper_baseline_v1"
--experiment-name "paper_optimized_v2"
--experiment-name "paper_final_v3"
```

### 4. Archivage

```bash
# Archiver les anciennes expÃ©riences
mkdir data/output/mlruns_archive
mv data/output/mlruns/<old_exp_id> data/output/mlruns_archive/
```

---

## ğŸ†˜ Troubleshooting

### ExpÃ©rience introuvable

```bash
# Lister toutes les expÃ©riences disponibles
python3 mlflow_explorer.py list

# VÃ©rifier le rÃ©pertoire
ls -la data/output/mlruns/
```

### Runs Ã©chouÃ©s

```bash
# Identifier les runs Ã©chouÃ©s
python3 mlflow_explorer.py runs <exp_id> --status FAILED

# Examiner les dÃ©tails
cat data/output/mlruns/<exp_id>/<run_id>/run_info.json
```

### Comparer des expÃ©riences incompatibles

Les expÃ©riences doivent avoir des algorithmes/datasets en commun pour Ãªtre comparables.

---

## ğŸ“š RÃ©sumÃ©

**Commandes essentielles :**

```bash
# Lancer benchmark
python3 run_full_benchmark.py --runs 5 --experiment-name "my_experiment"

# Explorer rÃ©sultats
python3 mlflow_explorer.py list
python3 mlflow_explorer.py show <exp_id>
python3 mlflow_explorer.py compare <exp1> <exp2>
```

**Structure :**
- `data/output/mlruns/<exp_id>/` : Tous les artefacts d'une expÃ©rience
- `experiment_meta.json` : MÃ©tadonnÃ©es
- `runs.json` : Tous les runs
- `summary.json` : Statistiques agrÃ©gÃ©es
- `<run_id>/` : Artefacts de chaque run

**Avantages MLflow :**
- âœ… TraÃ§abilitÃ© complÃ¨te
- âœ… Comparaison facile
- âœ… Statistiques automatiques
- âœ… Organisation claire
- âœ… Compatible avec MLflow UI

---

**ğŸ‰ SystÃ¨me de tracking complet pour vos benchmarks MATILDA !**
