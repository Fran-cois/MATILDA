# ğŸš€ Full Benchmark Automation

## Vue d'ensemble

`run_full_benchmark.py` automatise **tout le processus** :
1. âœ… ExÃ©cute tous les algorithmes sur tous les datasets
2. âœ… RÃ©pÃ¨te N fois chaque combinaison
3. âœ… Calcule automatiquement **moyenne Â± Ã©cart-type**
4. âœ… GÃ©nÃ¨re le tableau LaTeX avec statistiques
5. âœ… Sauvegarde rÃ©sultats et statistiques en JSON

**C'est la solution one-click pour benchmarker MATILDA !**

---

## ğŸ¯ Quick Start

### 1ï¸âƒ£ Option Simple : Arguments CLI

```bash
# Benchmark complet : 5 runs Ã— tous algorithmes Ã— tous datasets
python run_full_benchmark.py --runs 5

# Seulement MATILDA et SPIDER
python run_full_benchmark.py --runs 5 --algorithms MATILDA SPIDER

# Datasets spÃ©cifiques
python run_full_benchmark.py --runs 5 --datasets Bupa BupaImperfect

# Table simple au lieu de dÃ©taillÃ©e
python run_full_benchmark.py --runs 3 --table-type simple
```

### 2ï¸âƒ£ Option AvancÃ©e : Fichier de Configuration

```bash
# Utiliser benchmark_config.yaml
python run_full_benchmark.py --config benchmark_config.yaml

# Modifier benchmark_config.yaml selon vos besoins
nano benchmark_config.yaml
```

---

## ğŸ“‹ Exemples d'Usage

### Benchmark Rapide (Test)

```bash
# 3 runs, 2 algorithmes, 1 dataset = ~10 minutes
python run_full_benchmark.py --runs 3 --algorithms MATILDA SPIDER --datasets Bupa
```

**Quand utiliser :** Test rapide, vÃ©rification avant publication

### Benchmark Standard (Article)

```bash
# 5 runs, tous algorithmes, tous datasets = ~1-2 heures
python run_full_benchmark.py --runs 5
```

**Quand utiliser :** Article scientifique standard, statistiques robustes

### Benchmark Publication (Haute QualitÃ©)

```bash
# 10 runs, tous algorithmes = ~3-4 heures
python run_full_benchmark.py --runs 10
```

**Quand utiliser :** Publication prestigieuse, reviewers exigeants

### Benchmark CiblÃ©

```bash
# Comparer seulement MATILDA vs SPIDER sur datasets imperfects
python run_full_benchmark.py --runs 5 \
  --algorithms MATILDA SPIDER \
  --datasets BupaImperfect ImperfectTest
```

**Quand utiliser :** Comparaison spÃ©cifique, analyse ciblÃ©e

---

## ğŸ”§ Configuration YAML

### Fichier `benchmark_config.yaml`

```yaml
# Nombre de runs
runs: 5

# Algorithmes (commentez pour exclure)
algorithms:
  - MATILDA
  - SPIDER
  - ANYBURL
  - POPPER

# Datasets (commentez pour exclure)
datasets:
  - Bupa
  - BupaImperfect
  - ComparisonDataset
  - ImperfectTest

# Options
output_dir: data/output
timeout: 3600
table_type: detailed  # ou 'simple'
verbose: true
```

### Utilisation

```bash
# Charger depuis config
python run_full_benchmark.py --config benchmark_config.yaml

# Overrider certains paramÃ¨tres
python run_full_benchmark.py --config benchmark_config.yaml --runs 10
```

---

## ğŸ“Š Sorties GÃ©nÃ©rÃ©es

### 1. Fichier JSON des RÃ©sultats Bruts

**Fichier:** `data/output/full_benchmark_results_20260112_143020.json`

```json
{
  "MATILDA": {
    "Bupa": [
      { "rules": [...], "accuracy": 1.0, "time_total": 0.123 },
      { "rules": [...], "accuracy": 1.0, "time_total": 0.125 },
      ...
    ]
  }
}
```

### 2. Fichier JSON des Statistiques

**Fichier:** `data/output/full_benchmark_statistics_20260112_143020.json`

```json
{
  "MATILDA": {
    "Bupa": {
      "num_rules": { "mean": 9.0, "std": 0.0 },
      "accuracy": { "mean": 1.0, "std": 0.0 },
      "time_total": { "mean": 0.124, "std": 0.002 },
      "n_runs": 5
    }
  }
}
```

### 3. Table LaTeX avec Statistiques

**Fichier:** `data/output/benchmark_table_20260112_143020.tex`

```latex
\begin{table}[htbp]
\centering
\caption{Detailed Rule Discovery Performance with Statistics (5 runs)}
\resizebox{\textwidth}{!}{
\begin{tabular}{llrrrrrr}
\toprule
\textbf{Algorithm} & \textbf{Dataset} & \textbf{\#Rules} & \textbf{Acc.} & ...
\midrule
MATILDA & Bupa & $9 \pm 0.0$ & $1.000 \pm 0.000$ & ...
\bottomrule
\end{tabular}
}
\end{table}
```

### 4. RÃ©sumÃ© Console

```
BENCHMARK SUMMARY
============================================================

MATILDA:
  Bupa                :   9.0 Â±  0.0 rules (0.124 Â± 0.002s)
  BupaImperfect       :   9.0 Â±  0.0 rules (0.115 Â± 0.003s)
  ...

SPIDER:
  BupaImperfect       :  50.0 Â±  0.0 rules (0.089 Â± 0.001s)
  ...
```

---

## â±ï¸ Temps d'ExÃ©cution EstimÃ©

| Configuration | Runs | Algos | Datasets | Temps Total |
|---------------|------|-------|----------|-------------|
| Test rapide | 1 | 1 | 1 | ~2 min |
| Quick | 3 | 2 | 1 | ~10 min |
| Standard | 5 | 4 | 4 | ~1-2 heures |
| Publication | 10 | 4 | 4 | ~3-4 heures |

**Formule:** `Temps â‰ˆ runs Ã— algos Ã— datasets Ã— 1-3 minutes`

---

## ğŸ›ï¸ Options ComplÃ¨tes

```bash
python run_full_benchmark.py [OPTIONS]

Options:
  --runs N              Nombre de runs (dÃ©faut: 5)
  --algorithms A1 A2    Algorithmes (MATILDA, SPIDER, ANYBURL, POPPER)
  --datasets D1 D2      Datasets Ã  benchmarker
  --output-dir DIR      RÃ©pertoire de sortie (dÃ©faut: data/output)
  --timeout SECS        Timeout par run (dÃ©faut: 3600)
  --table-type TYPE     Type de table: simple ou detailed (dÃ©faut: detailed)
  --config FILE         Fichier de configuration YAML
  --quiet               Mode silencieux
  -h, --help            Afficher l'aide
```

---

## ğŸ’¡ Workflow RecommandÃ©

### Pour Article Scientifique

```bash
# 1. Test rapide (vÃ©rifier que tout marche)
python run_full_benchmark.py --runs 1 --algorithms MATILDA --datasets Bupa

# 2. Benchmark standard (rÃ©sultats article)
python run_full_benchmark.py --runs 5

# 3. Copier la table LaTeX dans votre article
cp data/output/benchmark_table_*.tex paper/tables/
```

### Pour PrÃ©sentation

```bash
# Benchmark rapide avec table simple
python run_full_benchmark.py --runs 3 --table-type simple \
  --algorithms MATILDA SPIDER
```

### Pour ExpÃ©rimentation

```bash
# Tester nouvelle feature sur dataset spÃ©cifique
python run_full_benchmark.py --runs 5 \
  --algorithms MATILDA \
  --datasets ImperfectTest
```

---

## ğŸ” RÃ©solution de ProblÃ¨mes

### ProblÃ¨me : Timeout sur certains algorithmes

**Solution :** Augmenter le timeout

```bash
python run_full_benchmark.py --runs 5 --timeout 7200  # 2 heures
```

### ProblÃ¨me : Trop long

**Solution :** RÃ©duire runs ou exclure algorithmes lents

```bash
# Exclure POPPER qui est lent
python run_full_benchmark.py --runs 5 --algorithms MATILDA SPIDER ANYBURL
```

### ProblÃ¨me : Certains runs Ã©chouent

**Solution :** Le script continue mÃªme si certains runs Ã©chouent. Les statistiques sont calculÃ©es sur les runs rÃ©ussis.

### ProblÃ¨me : MÃ©moire insuffisante

**Solution :** ExÃ©cuter par dataset

```bash
# Benchmark dataset par dataset
for dataset in Bupa BupaImperfect ComparisonDataset; do
  python run_full_benchmark.py --runs 5 --datasets $dataset
done
```

---

## ğŸ“ˆ Analyse des RÃ©sultats

### 1. Comparer Statistiques

```python
import json

with open("data/output/full_benchmark_statistics_*.json") as f:
    stats = json.load(f)

# Meilleure prÃ©cision
for algo in stats:
    for dataset in stats[algo]:
        acc = stats[algo][dataset]["accuracy"]
        print(f"{algo} on {dataset}: {acc['mean']:.3f} Â± {acc['std']:.3f}")
```

### 2. Analyser StabilitÃ©

```python
# Algorithmes avec faible Ã©cart-type = plus stables
for algo in stats:
    for dataset in stats[algo]:
        rules_std = stats[algo][dataset]["num_rules"]["std"]
        if rules_std > 1.0:
            print(f"âš ï¸  {algo} on {dataset} est instable (std={rules_std:.2f})")
```

### 3. Comparer Temps

```python
# Plus rapide
for algo in stats:
    total_time = sum(
        stats[algo][d]["time_total"]["mean"] 
        for d in stats[algo]
    )
    print(f"{algo}: {total_time:.3f}s total")
```

---

## ğŸ¯ Cas d'Usage Typiques

### 1. "Je veux benchmarker MATILDA pour mon article"

```bash
python run_full_benchmark.py --runs 5
```

â†’ ExÃ©cute tout, gÃ©nÃ¨re table LaTeX prÃªte pour publication

### 2. "Je veux comparer MATILDA vs SPIDER rapidement"

```bash
python run_full_benchmark.py --runs 3 --algorithms MATILDA SPIDER
```

â†’ Comparaison rapide (~15 minutes)

### 3. "Je teste une nouvelle feature de MATILDA"

```bash
python run_full_benchmark.py --runs 5 --algorithms MATILDA
```

â†’ Focus sur MATILDA uniquement

### 4. "Je veux des statistiques ultra-robustes"

```bash
python run_full_benchmark.py --runs 10
```

â†’ 10 runs = statistiques trÃ¨s fiables

---

## ğŸ†š Comparaison des Scripts

| Script | Usage | Vitesse | Statistiques |
|--------|-------|---------|--------------|
| `generate_latex_table.py` | Table depuis rÃ©sultats existants | âš¡âš¡âš¡ < 1s | âœ— |
| `run_benchmark.py` | Benchmark 1 algo, N runs | ğŸ¢ 5-30 min | âœ… |
| `run_full_benchmark.py` | **Benchmark TOUT, N runs** | ğŸ¢ğŸ¢ 1-4h | âœ… |

**Recommandation :**
- RÃ©sultats rapides â†’ `generate_latex_table.py`
- Test 1 algo â†’ `run_benchmark.py`
- **Article complet** â†’ `run_full_benchmark.py` â­

---

## âœ… Checklist Publication

- [ ] **ExÃ©cuter benchmark complet**
  ```bash
  python run_full_benchmark.py --runs 5
  ```

- [ ] **VÃ©rifier les statistiques**
  - Ã‰cart-type raisonnable (< 10% de la moyenne) ?
  - Nombre de runs suffisant (N â‰¥ 5) ?

- [ ] **GÃ©nÃ©rer table LaTeX**
  - Table gÃ©nÃ©rÃ©e automatiquement âœ“
  - Format professionnel (booktabs) âœ“

- [ ] **IntÃ©grer dans article**
  ```bash
  cp data/output/benchmark_table_*.tex paper/tables/results.tex
  ```

- [ ] **Documenter mÃ©thodologie**
  - Nombre de runs : 5
  - Algorithmes testÃ©s : MATILDA, SPIDER, ANYBURL, POPPER
  - Datasets : Bupa, BupaImperfect, ComparisonDataset, ImperfectTest
  - Machine : [spÃ©cifier]
  - Timeout : 1h par run

---

## ğŸ“ Pour en savoir plus

- `WHICH_SCRIPT.md` - Guide pour choisir le bon script
- `LATEX_TABLES_GUIDE.md` - Guide complet LaTeX
- `benchmark_config.yaml` - Configuration exemple
- `README.md` - Documentation gÃ©nÃ©rale

---

**ğŸ‰ Avec `run_full_benchmark.py`, benchmarker tous les algorithmes et gÃ©nÃ©rer le tableau LaTeX est aussi simple qu'une seule commande !**
