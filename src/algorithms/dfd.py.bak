# algorithms/dfd.py

import ast
import os
import logging
import pandas as pd
import itertools
from collections import defaultdict
from datetime import datetime

from algorithms.base_algorithm import BaseAlgorithm
from utils.rules import FunctionalDependency, Rule
from utils.run_cmd import run_cmd


class DFD(BaseAlgorithm):
    """
    Implémentation de l'interface pour l'algorithme DFD (Depth-First Discovery) pour la découverte de dépendances fonctionnelles.
    Cette implémentation utilise un JAR Java externe avec une solution de repli Python en cas d'échec.
    """

    def verify_csv_file(self, file_path: str) -> bool:
        """
        Vérifie si le fichier CSV est valide et contient des données.
        """
        try:
            # Vérifier si le fichier existe
            if not os.path.exists(file_path):
                logging.error(f"Le fichier n'existe pas: {file_path}")
                return False
                
            # Vérifier si le fichier n'est pas vide
            if os.path.getsize(file_path) == 0:
                logging.error(f"Le fichier est vide: {file_path}")
                return False
                
            # Essayer de lire le fichier avec pandas
            df = pd.read_csv(file_path)
            
            # Vérifier s'il y a des colonnes
            if len(df.columns) == 0:
                logging.error(f"Le fichier n'a pas de colonnes: {file_path}")
                return False
                
            # Vérifier s'il y a des données
            if len(df) == 0:
                logging.error(f"Le fichier n'a pas de données: {file_path}")
                return False
                
            # Vérifier si toutes les colonnes ont des noms valides
            if df.columns.isnull().any():
                logging.error(f"Le fichier contient des noms de colonnes invalides: {file_path}")
                return False
                
            logging.info(f"Fichier CSV valide: {file_path}")
            logging.info(f"Nombre de colonnes: {len(df.columns)}")
            logging.info(f"Nombre de lignes: {len(df)}")
            logging.info(f"Colonnes: {list(df.columns)}")
            
            return True
            
        except Exception as e:
            logging.error(f"Erreur lors de la vérification du fichier CSV {file_path}: {str(e)}")
            return False

    def discover_rules(self, **kwargs) -> Rule:
        """
        Découvre les dépendances fonctionnelles.
        Essaie d'abord d'utiliser l'implémentation Java de l'algorithme DFD via le JAR externe.
        En cas d'échec, utilise une implémentation Python simplifiée.
        """
        rules = {}
        use_fallback = kwargs.get('use_fallback', False)
        
        if not use_fallback:
            logging.info("Lancement de l'algorithme DFD (version Java)")
            try_java = self._discover_rules_java(**kwargs)
            if try_java:
                return try_java
            logging.warning("L'exécution de DFD (Java) a échoué, utilisation de l'implémentation Python simplifiée")
        
        # Utiliser l'implémentation Python simplifiée
        logging.info("Lancement de l'algorithme DFD (version Python simplifiée)")
        return self._discover_rules_python(**kwargs)
    
    def _discover_rules_java(self, **kwargs) -> Rule:
        """
        Implémentation de la découverte de dépendances fonctionnelles via DFD (JAR Java).
        """
        rules = {}
        
        # Récupération des paramètres (pour l'information, ne pas les utiliser dans la commande)
        max_lhs_size = kwargs.get('max_lhs_size', 3)  
        min_conf = kwargs.get('min_confidence', 0.9)  
        time_limit = kwargs.get('time_limit', 600)    
        
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            algorithm_name = "DFD"
            classPath = "de.metanome.algorithms.dfd.DFD"  # Correction du nom de classe
            rule_type = "fds"
            # Utiliser uniquement les options reconnues
            params = " --file-key INPUT_FILES"

            # Obtenir la liste des fichiers CSV
            csv_files = [
                os.path.join(self.database.base_csv_dir, f)
                for f in os.listdir(self.database.base_csv_dir)
                if f.endswith('.csv')
            ]
            
            logging.info(f"Traitement de {len(csv_files)} fichiers CSV individuellement")
            
            # Traiter chaque fichier séparément pour éviter les problèmes
            for csv_file in csv_files:
                table_name = os.path.basename(csv_file).replace('.csv', '')
                logging.info(f"Traitement du fichier: {csv_file} (table: {table_name})")
                
                current_time = datetime.now()
                jar_path = f"{script_dir}/bins/metanome/"
                file_name = f'{current_time.strftime("%Y-%m-%d_%H-%M-%S")}_{algorithm_name}_{table_name}'
                
                # Ajouter tous les JAR du répertoire dans le classpath
                all_jars = [f"{jar_path}{jar_file}" for jar_file in os.listdir(jar_path) if jar_file.endswith('.jar')]
                classpath = ":".join(all_jars)
                
                cmd_string = (
                    f"""java -Xmx4g -cp {classpath} """
                    f"""de.metanome.cli.App --algorithm {classPath} --files {csv_file} """
                    f"""--file-key INPUT_FILES --separator "," --header """
                    f"""--output file:{file_name}"""
                )
                
                # Précharger le fichier CSV pour éviter le NullPointerException
                try:
                    # Vérifier le contenu du fichier CSV
                    import pandas as pd
                    df = pd.read_csv(csv_file)
                    logging.info(f"Fichier CSV préchargé avec succès. Colonnes: {list(df.columns)}")
                    
                    if len(df) == 0:
                        logging.error(f"Le fichier CSV {csv_file} est vide")
                        continue
                except Exception as e:
                    logging.error(f"Erreur lors du préchargement du fichier CSV {csv_file}: {str(e)}")
                    continue
                    
                cmd_string = (
                    f"""java -Xmx4g -cp {jar_path}*.jar """
                    f"""de.metanome.cli.App --algorithm {classPath} --files {csv_file} """
                    f"""--file-key INPUT_FILES --separator "," --header """
                    f"""--output file:{file_name}"""
                )
                
                logging.info(f"Exécution de la commande: {cmd_string}")
                
                if not run_cmd(cmd_string):
                    logging.error(f"Échec de l'exécution de DFD pour {table_name}")
                    continue
                    
                # Traiter les résultats pour ce fichier
                result_file_path = os.path.join("results", f"{file_name}_{rule_type}")
                
                try:
                    with open(result_file_path, mode="r") as f:
                        raw_rules = [line for line in f if line.strip()]
                except FileNotFoundError:
                    logging.error(f"Fichier de résultats non trouvé: {result_file_path}")
                    continue

                if os.path.exists(result_file_path):
                    os.remove(result_file_path)

                # Traiter les règles de ce fichier
                for raw_rule in raw_rules:
                    try:
                        raw_rule = ast.literal_eval(raw_rule)
                    except (ValueError, SyntaxError) as e:
                        logging.warning(f"Format de règle invalide: {raw_rule} - {e}")
                        continue  # Ignorer les formats de règle invalides

                    try:
                        # Parse les dépendances fonctionnelles depuis le format JSON retourné par DFD
                        determinant_columns = tuple(
                            col["columnIdentifier"] for col in raw_rule.get("determinant", {}).get("columnIdentifiers", [])
                        )
                        
                        dependant_columns = tuple(
                            col["columnIdentifier"] for col in raw_rule.get("dependant", {}).get("columnIdentifiers", [])
                        )
                        
                        # Récupération de la confiance si disponible
                        confidence = raw_rule.get("confidence", 1.0)
                        
                        if determinant_columns and dependant_columns:
                            fd = FunctionalDependency(
                                table_dependant=table_name,
                                columns_dependant=determinant_columns,
                                table_referenced=table_name,
                                columns_referenced=dependant_columns,
                                table=table_name
                            )
                            rules[fd] = (1.0, confidence)  # Support à 1.0, confiance selon la règle
                            logging.info(f"Dépendance fonctionnelle découverte: {fd} (confiance: {confidence})")
                    except (KeyError, IndexError, AttributeError) as e:
                        logging.warning(f"Données de règle malformées: {raw_rule} - {e}")
                        continue  # Ignorer les données de règle malformées
            
            logging.info(f"Découvert {len(rules)} dépendances fonctionnelles au total")
            return rules if rules else None
                
        except Exception as e:
            logging.error(f"Erreur lors de la découverte des dépendances fonctionnelles via Java: {str(e)}")
            return None
    
    def _discover_rules_python(self, **kwargs) -> Rule:
        """
        Implémentation Python simplifiée pour découvrir les dépendances fonctionnelles.
        Utilise une approche simple par paires de colonnes.
        """
        import pandas as pd
        import itertools
        from collections import defaultdict
        
        rules = {}
        
        try:
            # Récupération des paramètres
            max_lhs_size = kwargs.get('max_lhs_size', 1)  # Par défaut, limiter à des dépendances simples
            
            # Obtenir la liste des fichiers CSV
            csv_files = [
                os.path.join(self.database.base_csv_dir, f)
                for f in os.listdir(self.database.base_csv_dir)
                if f.endswith('.csv')
            ]
            
            logging.info(f"Traitement de {len(csv_files)} fichiers CSV avec l'implémentation Python")
            
            # Traiter chaque fichier séparément
            for csv_file in csv_files:
                if not self.verify_csv_file(csv_file):
                    logging.warning(f"Fichier CSV invalide, ignoré: {csv_file}")
                    continue
                
                table_name = os.path.basename(csv_file).replace('.csv', '')
                logging.info(f"Traitement du fichier: {csv_file} (table: {table_name})")
                
                try:
                    # Lire le fichier CSV avec pandas
                    df = pd.read_csv(csv_file)
                    
                    # Découvrir les dépendances fonctionnelles pour ce fichier
                    file_fds = self._discover_fds_simple(df, table_name, max_lhs_size)
                    
                    # Ajouter les dépendances découvertes au résultat global
                    for fd in file_fds:
                        rules[fd] = (1, 1)  # Support et confiance à 1 pour les dépendances exactes
                
                except Exception as e:
                    logging.error(f"Erreur lors du traitement du fichier {csv_file}: {str(e)}")
                    continue
            
            logging.info(f"Découvert {len(rules)} dépendances fonctionnelles au total avec l'implémentation Python")
            return rules
                
        except Exception as e:
            logging.error(f"Erreur lors de la découverte des dépendances fonctionnelles via Python: {str(e)}")
            return {}
            
    def _discover_fds_simple(self, df, table_name, max_lhs_size=1):
        """
        Méthode simplifiée pour découvrir les dépendances fonctionnelles.
        Cette implémentation vérifie les dépendances avec au plus max_lhs_size colonnes dans le déterminant.
        """
        import itertools
        from collections import defaultdict
        
        discovered_fds = []
        columns = list(df.columns)
        
        # Pour chaque taille possible du déterminant jusqu'à max_lhs_size
        for lhs_size in range(1, min(max_lhs_size + 1, len(columns))):
            # Pour chaque combinaison possible de colonnes comme déterminant
            for lhs_cols in itertools.combinations(columns, lhs_size):
                # Pour chaque colonne possible comme dépendante
                for rhs_col in columns:
                    # Ignorer si la colonne dépendante est dans le déterminant
                    if rhs_col in lhs_cols:
                        continue
                    
                    # Créer un dictionnaire pour regrouper les valeurs de rhs_col par valeurs de lhs_cols
                    dependency_dict = defaultdict(set)
                    
                    # Remplir le dictionnaire avec les valeurs
                    for _, row in df.iterrows():
                        # Créer une clé composite pour les valeurs du déterminant
                        lhs_key = tuple(str(row[col]) for col in lhs_cols)
                        rhs_val = str(row[rhs_col])
                        dependency_dict[lhs_key].add(rhs_val)
                    
                    # Si pour chaque valeur distincte de lhs_cols, il y a exactement une valeur de rhs_col,
                    # alors lhs_cols -> rhs_col est une dépendance fonctionnelle
                    is_fd = all(len(values) == 1 for values in dependency_dict.values())
                    
                    if is_fd:
                        try:
                            fd = FunctionalDependency(
                                table_dependant=table_name,
                                columns_dependant=lhs_cols,
                                table_referenced=table_name,
                                columns_referenced=(rhs_col,),
                                table=table_name
                            )
                            discovered_fds.append(fd)
                            logging.info(f"Dépendance fonctionnelle découverte: {table_name}.{','.join(lhs_cols)} -> {table_name}.{rhs_col}")
                        except Exception as e:
                            logging.error(f"Erreur lors de la création de la dépendance fonctionnelle: {e}")
        
        return discovered_fds
