#!/usr/bin/env python3
"""
Quick Reference Guide: Enhanced Metrics Comparison System

A fast lookup guide for using the new metrics comparison features.
"""

# ============================================================================
# QUICK START
# ============================================================================

"""
1. Compute MATILDA metrics for all algorithms:

   python3 compute_anyburl_metrics.py anyburl_Bupa_example_results.json
   python3 compute_popper_metrics.py popper_Bupa_example_results.json
   python3 compute_spider_metrics.py spider_Bupa_example_results.json
   python3 compute_amie3_metrics.py amie3_Bupa_example_results.json

2. Generate comparison report:

   python3 compare_bupa_metrics.py

3. Review output with:
   - Original Algorithm Metrics (what algorithms reported)
   - MATILDA-Computed Metrics (database validation)
   - Comparative Insights (differences and analysis)
"""

# ============================================================================
# CONSTANTS REFERENCE
# ============================================================================

"""
Import constants in your code:

    from metrics_constants import (
        MIN_CONFIDENCE_THRESHOLD,           # 0.5
        HIGH_CONFIDENCE_THRESHOLD,          # 0.8
        EXCELLENT_CONFIDENCE_THRESHOLD,     # 0.9
        MIN_SUPPORT_THRESHOLD,              # 0.1
        GOOD_SUPPORT_THRESHOLD,             # 0.5
        EXCELLENT_SUPPORT_THRESHOLD,        # 0.8
        MIN_ACCURACY_THRESHOLD,             # 0.6
        GOOD_ACCURACY_THRESHOLD,            # 0.8
        EXCELLENT_ACCURACY_THRESHOLD,       # 0.95
        ANYBURL_DEFAULT_CONFIDENCE_WEIGHT,  # 0.6
        ANYBURL_DEFAULT_SUPPORT_WEIGHT,     # 0.4
        EXAMPLE_RESULTS_PATTERN,            # "*_example_results.json"
        WITH_METRICS_PATTERN,               # "*_with_metrics*.json"
    )

Use in your code:

    if rule_confidence >= HIGH_CONFIDENCE_THRESHOLD:
        print("Rule has high confidence")
    
    weighted = (
        confidence * ANYBURL_DEFAULT_CONFIDENCE_WEIGHT +
        support * ANYBURL_DEFAULT_SUPPORT_WEIGHT
    )
"""

# ============================================================================
# METRICS TYPES
# ============================================================================

"""
THREE TYPES OF METRICS AVAILABLE:

1. ORIGINAL METRICS (from algorithm output):
   - Stored in: *_example_results.json files
   - What: Self-reported metrics from the discovery algorithm
   - When: Generated by SPIDER/POPPER/AMIE3/AnyBURL
   - Shows: What algorithm claims about rule quality

2. MATILDA-COMPUTED METRICS (from database validation):
   - Stored in: *_with_metrics*.json files
   - What: Metrics computed by MATILDA validators
   - When: Generated by compute_*_metrics.py scripts
   - Shows: What database actually validates about rules

3. COMPARATIVE METRICS (differences):
   - Calculated: During report generation
   - What: Difference between original and MATILDA
   - Shows: How reliable is the algorithm's self-reporting

METRIC FIELDS:
- accuracy:    Correctness/validity (0.0-1.0)
- confidence:  Applicability/precision (0.0-1.0)
- support:     Coverage (0.0-1.0)
- correct:     Boolean validity
- compatible:  Boolean compatibility
"""

# ============================================================================
# ALGORITHM CHARACTERISTICS
# ============================================================================

"""
SPIDER (Inclusion Dependencies):
- Type: IND rules (table[col1] â†’ table[col2])
- Original Metrics: confidence, support, correct, compatible
- No Accuracy: Uses correctness flag instead
- MATILDA Validation: Table existence check

POPPER (Inductive Logic Programming):
- Type: TGD rules (Head :- Body)
- Original Metrics: accuracy, confidence, support, correct, compatible
- Has Accuracy: Full metric set available
- MATILDA Validation: Database consistency check

AMIE3 (Association Rule Mining):
- Type: Horn rules (Head :- Body)
- Original Metrics: confidence, support (no accuracy)
- No Accuracy: Not computed by AMIE3
- MATILDA Validation: Database coverage analysis

AnyBURL (Markov Logic Networks):
- Type: TGD rules (Head :- Body)
- Original Metrics: confidence (accuracy field = -1.0 for invalid)
- No Accuracy: -1.0 indicates invalid
- MATILDA Validation: Rule compatibility check
"""

# ============================================================================
# ARCHITECTURE PATTERNS
# ============================================================================

"""
ADDING A NEW ALGORITHM:

1. Create a comparator class:

    class MyAlgoComparator(BaseComparator):
        \"\"\"Handle MyAlgo-specific metrics.\"\"\"
        
        def extract_metrics(self, rules: List[Dict]) -> MetricStats:
            stats = MetricStats(rules_count=len(rules))
            
            # Extract your algorithm's metrics
            for rule in rules:
                # Your extraction logic
                pass
            
            stats.original_confidence = ...
            stats.original_support = ...
            stats.original_accuracy = ...  # if applicable
            
            return stats

2. Register the comparator:

    ComparatorRegistry.register('myalgo', MyAlgoComparator)

3. Done! It will automatically appear in comparisons.

REGISTRY PATTERN BENEFITS:
- Add algorithms without modifying comparison code
- Each algorithm handles its own metric quirks
- Type-safe extraction with MetricStats
- Easy to test individual comparators
"""

# ============================================================================
# METRIC INTERPRETATION
# ============================================================================

"""
CONFIDENCE (Rule Applicability):
- 0.9-1.0:  Excellent - Rule applies consistently
- 0.8-0.9:  High - Rule is well-founded
- 0.5-0.8:  Moderate - Rule has some merit
- <0.5:     Low - Rule needs validation

SUPPORT (Rule Coverage):
- 0.8-1.0:  Excellent - Rule covers most data
- 0.5-0.8:  Good - Rule has fair coverage
- 0.1-0.5:  Moderate - Limited coverage
- <0.1:     Low - Rare cases only

ACCURACY (Rule Validity):
- 0.95+:    Excellent - Rule is very precise
- 0.8-0.95: Good - Rule is mostly valid
- 0.6-0.8:  Moderate - Rule needs validation
- <0.6:     Low - Rule is unreliable

DIFFERENCES (Original vs MATILDA):
- -50% to +50%:   Expected variance (algorithm estimates vary)
- -100% or Â±100%+: Major discrepancy (algorithm self-reporting unreliable)
- 0%:              Algorithm matches database truth perfectly
"""

# ============================================================================
# COMMON TASKS
# ============================================================================

"""
TASK: Check if AnyBURL metrics are properly weighted

    from metrics_constants import ANYBURL_DEFAULT_CONFIDENCE_WEIGHT
    
    confidence_weight = ANYBURL_DEFAULT_CONFIDENCE_WEIGHT  # 0.6
    support_weight = 1.0 - confidence_weight               # 0.4
    
    score = confidence * confidence_weight + support * support_weight


TASK: Find rules above a quality threshold

    from metrics_constants import HIGH_CONFIDENCE_THRESHOLD
    
    high_quality_rules = [
        rule for rule in rules 
        if rule.confidence >= HIGH_CONFIDENCE_THRESHOLD
    ]


TASK: Compare MATILDA accuracy vs original

    if algo.original_accuracy and algo.matilda_accuracy:
        diff = algo.matilda_accuracy - algo.original_accuracy
        diff_pct = (diff / algo.original_accuracy) * 100
        print(f"Accuracy changed by {diff_pct:.1f}%")


TASK: Find algorithms with most reliable self-reporting

    # Low difference = good self-reporting
    reliability = {}
    for algo, stats in metrics.items():
        if stats.original_confidence > 0:
            diff = abs(stats.matilda_confidence - stats.original_confidence)
            reliability[algo] = diff / stats.original_confidence
    
    most_reliable = min(reliability, key=reliability.get)
"""

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

"""
PROBLEM: "No metrics file found for [algorithm]"
SOLUTION: 
- Run compute_[algorithm]_metrics.py script first
- Check if results files exist in current directory or data/output/
- Ensure file naming matches pattern: [algorithm]_[database]_*

PROBLEM: "MATILDA metrics not available"
SOLUTION:
- Run compute_*_metrics.py for all algorithms
- Wait for files to be generated (check data/output/)
- Ensure database files are in correct location

PROBLEM: Constants not importing
SOLUTION:
- Verify metrics_constants.py is in MATILDA directory
- Check Python path includes current directory
- Ensure no import errors in metrics_constants.py

PROBLEM: Metrics seem wrong (all zeros)
SOLUTION:
- Check if validation database is available
- Verify database contains expected tables
- Review compute_*_metrics.py logs for errors
- Try with a known-good database (Bupa)

PROBLEM: Different metrics each time
SOLUTION:
- Multiple files may be found for same algorithm/database
- Script selects most recent by modification time
- Delete old with_metrics files if they're stale
- Regenerate fresh metrics files
"""

# ============================================================================
# FILE LOCATIONS & PATTERNS
# ============================================================================

"""
ORIGINAL ALGORITHM RESULTS:
- Location: Current directory or data/output/
- Pattern: [algorithm]_[database]_example_results.json
- Examples:
  - spider_Bupa_example_results.json
  - popper_Bupa_example_results.json
  - amie3_Bupa_example_results.json
  - anyburl_Bupa_example_results.json

MATILDA-COMPUTED METRICS:
- Location: data/output/
- Pattern: [algorithm]_[database]_example_results_with_metrics_TIMESTAMP.json
- Examples:
  - spider_Bupa_example_results_with_metrics_2026-01-15_10-53-00.json
  - popper_Bupa_example_results_with_metrics_2026-01-15_10-52-57.json
  - amie3_BupaImperfect_results_with_metrics_2026-01-15_10-53-03.json
  - anyburl_Bupa_example_results_with_metrics_2026-01-15_10-52-54.json

REPORT MARKDOWN FILES:
- Location: data/output/
- Pattern: [algorithm]_[database]_*_with_metrics_TIMESTAMP.md
- Contains: Human-readable metrics summary
"""

# ============================================================================
# QUICK COMMAND REFERENCE
# ============================================================================

"""
# Generate metrics for all algorithms at once
for algo in spider popper amie3 anyburl; do
    python3 compute_${algo}_metrics.py ${algo}_Bupa_example_results.json
done

# Generate comparison report
python3 compare_bupa_metrics.py

# View just the tables (no logs)
python3 compare_bupa_metrics.py 2>/dev/null | grep -A 20 "ORIGINAL"

# Check what metrics files exist
ls -lh data/output/*_with_metrics*.json

# View MATILDA metrics for specific file
cat data/output/spider_Bupa_example_results_with_metrics*.json | python3 -m json.tool

# Count rules in metrics file
python3 -c "import json; data=json.load(open('file.json')); print(f'Rules: {len(data)}')"
"""

# ============================================================================
# DOCUMENTATION LINKS
# ============================================================================

"""
ðŸ“– DOCUMENTATION FILES:

1. metrics_constants.py (52 lines)
   - All configuration constants
   - Import here for threshold values
   - Location: MATILDA/

2. METRICS_CONSTANTS_GUIDE.py (170 lines)
   - Detailed explanation of each constant
   - Usage examples
   - Naming conventions
   - Location: MATILDA/

3. METRICS_COMPARISON_REPORT.md (280 lines)
   - User guide and feature overview
   - Architecture explanation
   - Complete usage examples
   - Key insights from Bupa dataset
   - Location: MATILDA/

4. IMPLEMENTATION_SUMMARY.md (300 lines)
   - Technical implementation details
   - Architecture patterns
   - Test results
   - Next steps for extension
   - Location: MATILDA/

5. COMPLETION_CHECKLIST.md (This directory)
   - Complete checklist of implemented features
   - Test results for all algorithms
   - File list and modifications
   - Future enhancement ideas
   - Location: MATILDA/
"""

# ============================================================================
# SUPPORT & EXTENSION
# ============================================================================

"""
NEED TO EXTEND THE SYSTEM?

1. Add New Algorithm:
   - Create Comparator subclass (see ARCHITECTURE PATTERNS)
   - Implement extract_metrics() method
   - Register with ComparatorRegistry.register()
   
2. Add New Constants:
   - Edit metrics_constants.py
   - Follow naming convention: ALGORITHM_PARAMETER_NAME
   - Add documentation comment
   - Import in modules that use it
   
3. Add New Analysis:
   - Extend MetricsAnalyzer class
   - Add new method for analysis (e.g., generate_statistical_insights())
   - Call from main() function
   - Document in METRICS_COMPARISON_REPORT.md

4. Add Export Format:
   - Create new export method in MetricsAnalyzer
   - Support CSV, LaTeX, HTML, etc.
   - Use MetricStats data
   - Document in guide

NEED HELP?
- Check METRICS_CONSTANTS_GUIDE.py for constant details
- Review source code comments
- Look at algorithm-specific comparators for examples
- Check IMPLEMENTATION_SUMMARY.md for architecture details
"""

if __name__ == "__main__":
    print(__doc__)
    print("\nâœ“ Quick Reference Guide loaded!")
    print("  See docstrings above for fast lookup information.")

