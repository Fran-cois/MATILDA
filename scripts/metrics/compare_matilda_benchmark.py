#!/usr/bin/env python3
"""
Benchmark Comparison: MATILDA vs Other Algorithms
Compares coverage (rule matching) and speed (execution time) between MATILDA and baseline algorithms.
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import statistics

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from compute_coverage_metrics import RuleMatcher


def load_mlflow_experiment(experiment_dir: Path) -> Dict:
    """Load MLflow experiment data."""
    runs = []
    
    # Find all run directories
    for run_dir in experiment_dir.iterdir():
        if not run_dir.is_dir() or run_dir.name in ['experiment_meta.json', 'summary.json', 'coverage_metrics.json']:
            continue
        
        # Load run data
        params_file = run_dir / 'params.json'
        metrics_file = run_dir / 'metrics.json'
        rules_file = run_dir / 'rules.json'
        
        if params_file.exists() and metrics_file.exists():
            with open(params_file) as f:
                params = json.load(f)
            with open(metrics_file) as f:
                metrics = json.load(f)
            
            rules = []
            if rules_file.exists():
                with open(rules_file) as f:
                    rules = json.load(f)
            
            runs.append({
                'run_id': run_dir.name,
                'params': params,
                'metrics': metrics,
                'rules': rules
            })
    
    return {'runs': runs}


def aggregate_runs_by_algo_dataset(runs: List[Dict]) -> Dict:
    """Aggregate multiple runs by algorithm and dataset."""
    aggregated = {}
    
    for run in runs:
        algo = run['params']['algorithm']
        dataset = run['params']['dataset']
        key = (algo, dataset)
        
        if key not in aggregated:
            aggregated[key] = {
                'algorithm': algo,
                'dataset': dataset,
                'runs': [],
                'all_rules': []
            }
        
        aggregated[key]['runs'].append(run)
        if run['rules']:
            aggregated[key]['all_rules'].extend(run['rules'])
    
    return aggregated


def compute_speed_metrics(runs: List[Dict]) -> Dict:
    """Compute speed statistics from multiple runs."""
    durations = [r['metrics'].get('duration_seconds', 0) for r in runs]
    
    if not durations or all(d == 0 for d in durations):
        return {
            'mean': 0,
            'std': 0,
            'min': 0,
            'max': 0,
            'count': len(durations)
        }
    
    return {
        'mean': statistics.mean(durations),
        'std': statistics.stdev(durations) if len(durations) > 1 else 0,
        'min': min(durations),
        'max': max(durations),
        'count': len(durations)
    }


def compute_coverage_between_algorithms(matilda_rules: List[Dict], other_rules: List[Dict], 
                                        other_algo: str) -> Tuple[float, float]:
    """
    Compute coverage metrics between MATILDA and another algorithm.
    
    Also computes MATILDA-validated accuracy/coverage for each competitor rule
    (TODO: This is a placeholder - full implementation would require database access)
    
    Returns:
        (match_percentage, completeness_percentage)
        - match_percentage: % of other_algo rules that match MATILDA rules
        - completeness_percentage: % of joinable other_algo rules that MATILDA recovered
    """
    if not other_rules:
        return 0.0, 0.0
    
    matcher = RuleMatcher(verbose=False)
    
    # TODO: Add database access to compute MATILDA accuracy/coverage for competitor rules
    # For now, we store original metrics and can compare them in reports
    # Each rule should have: original_accuracy, original_coverage (from the algorithm)
    # and we could compute: matilda_validated_accuracy, matilda_validated_coverage
    
    # Segment 1: Rules Match - how many other_algo rules match MATILDA
    matches = 0
    for other_rule in other_rules:
        other_tgd = other_rule.get('tgd', '')
        if not other_tgd:
            continue
        
        # Store matching information for later analysis
        other_rule['matches_matilda'] = False
        
        for matilda_rule in matilda_rules:
            matilda_tgd = matilda_rule.get('tgd', '')
            if not matilda_tgd:
                continue
            
            if matcher.tgd_matches_horn(matilda_rule, other_rule):
                matches += 1
                other_rule['matches_matilda'] = True
                break
    
    match_pct = (matches / len(other_rules)) * 100 if other_rules else 0.0
    
    # Segment 2: Completeness under joinability constraint
    joinable_other_rules = [r for r in other_rules if matcher.is_joinable(r, other_algo)]
    
    if not joinable_other_rules:
        completeness_pct = 0.0
    else:
        joinable_matches = 0
        for other_rule in joinable_other_rules:
            other_tgd = other_rule.get('tgd', '')
            for matilda_rule in matilda_rules:
                matilda_tgd = matilda_rule.get('tgd', '')
                if matcher.tgd_matches_horn(matilda_rule, other_rule):
                    joinable_matches += 1
                    break
        
        completeness_pct = (joinable_matches / len(joinable_other_rules)) * 100
    
    return match_pct, completeness_pct


def extract_metrics_summary(rules: List[Dict]) -> Dict[str, float]:
    """
    Extract summary statistics of original_accuracy and original_coverage from rules.
    
    Args:
        rules: List of rule dictionaries with original_accuracy and original_coverage fields
        
    Returns:
        Dictionary with mean/std/min/max for original metrics
    """
    if not rules:
        return {
            'original_accuracy_mean': 0.0,
            'original_accuracy_std': 0.0,
            'original_coverage_mean': 0.0,
            'original_coverage_std': 0.0,
            'num_rules': 0
        }
    
    original_accuracies = [r.get('original_accuracy', 0) for r in rules if r.get('original_accuracy') is not None]
    original_coverages = [r.get('original_coverage', 0) for r in rules if r.get('original_coverage') is not None]
    
    return {
        'original_accuracy_mean': statistics.mean(original_accuracies) if original_accuracies else 0.0,
        'original_accuracy_std': statistics.stdev(original_accuracies) if len(original_accuracies) > 1 else 0.0,
        'original_coverage_mean': statistics.mean(original_coverages) if original_coverages else 0.0,
        'original_coverage_std': statistics.stdev(original_coverages) if len(original_coverages) > 1 else 0.0,
        'num_rules': len(rules)
    }


def generate_comparison_report(aggregated_data: Dict, output_dir: Path):
    """Generate comparison report between MATILDA and other algorithms."""
    
    # Group by dataset
    datasets = set(key[1] for key in aggregated_data.keys())
    algorithms = set(key[0] for key in aggregated_data.keys())
    
    # Remove MATILDA from comparison algorithms
    other_algorithms = sorted(algorithms - {'MATILDA'})
    
    # Prepare comparison data
    comparison_results = []
    
    for dataset in sorted(datasets):
        matilda_key = ('MATILDA', dataset)
        if matilda_key not in aggregated_data:
            continue
        
        matilda_data = aggregated_data[matilda_key]
        matilda_rules = matilda_data['all_rules']
        matilda_speed = compute_speed_metrics(matilda_data['runs'])
        matilda_num_rules = len(matilda_rules)
        
        for other_algo in other_algorithms:
            other_key = (other_algo, dataset)
            if other_key not in aggregated_data:
                continue
            
            other_data = aggregated_data[other_key]
            other_rules = other_data['all_rules']
            other_speed = compute_speed_metrics(other_data['runs'])
            other_num_rules = len(other_rules)
            
            # Compute coverage
            match_pct, completeness_pct = compute_coverage_between_algorithms(
                matilda_rules, other_rules, other_algo
            )
            
            # Speed comparison (speedup factor)
            if matilda_speed['mean'] > 0 and other_speed['mean'] > 0:
                speedup = other_speed['mean'] / matilda_speed['mean']
            else:
                speedup = 0
            
            # Extract original metrics from competitor rules
            other_metrics = extract_metrics_summary(other_rules)
            matilda_metrics = extract_metrics_summary(matilda_rules)
            
            comparison_results.append({
                'dataset': dataset,
                'other_algorithm': other_algo,
                'matilda_rules': matilda_num_rules,
                'other_rules': other_num_rules,
                'match_pct': match_pct,
                'completeness_pct': completeness_pct,
                'matilda_time': matilda_speed['mean'],
                'matilda_time_std': matilda_speed['std'],
                'other_time': other_speed['mean'],
                'other_time_std': other_speed['std'],
                'speedup': speedup,
                # Add original metrics
                'other_original_accuracy': other_metrics['original_accuracy_mean'],
                'other_original_coverage': other_metrics['original_coverage_mean'],
                'matilda_original_accuracy': matilda_metrics['original_accuracy_mean'],
                'matilda_original_coverage': matilda_metrics['original_coverage_mean']
            })
    
    # Generate Markdown report
    markdown_report = generate_markdown_report(comparison_results)
    md_file = output_dir / 'MATILDA_COMPARISON_REPORT.md'
    with open(md_file, 'w') as f:
        f.write(markdown_report)
    print(f"ðŸ“„ Markdown report: {md_file}")
    
    # Generate LaTeX table
    latex_table = generate_latex_comparison_table(comparison_results)
    tex_file = output_dir / 'matilda_comparison_table.tex'
    with open(tex_file, 'w') as f:
        f.write(latex_table)
    print(f"ðŸ“„ LaTeX table: {tex_file}")
    
    # Generate JSON data
    json_file = output_dir / 'matilda_comparison_data.json'
    with open(json_file, 'w') as f:
        json.dump(comparison_results, f, indent=2)
    print(f"ðŸ“„ JSON data: {json_file}")
    
    return comparison_results


def generate_markdown_report(results: List[Dict]) -> str:
    """Generate Markdown comparison report."""
    md = "# MATILDA Benchmark Comparison\n\n"
    md += "Comparison of MATILDA against baseline algorithms on **Coverage** and **Speed**.\n\n"
    md += "## Metrics Explained\n\n"
    md += "- **Coverage (Match %)**: Percentage of baseline algorithm rules that match MATILDA rules\n"
    md += "- **Coverage (Completeness %)**: Percentage of joinable baseline rules recovered by MATILDA\n"
    md += "- **Speed (Speedup)**: How much faster MATILDA is compared to baseline (speedup > 1 = MATILDA faster)\n\n"
    
    # Group by dataset
    datasets = set(r['dataset'] for r in results)
    
    for dataset in sorted(datasets):
        md += f"## Dataset: {dataset}\n\n"
        
        dataset_results = [r for r in results if r['dataset'] == dataset]
        
        if not dataset_results:
            md += "*No results available*\n\n"
            continue
        
        # Coverage comparison
        md += "### Coverage Comparison\n\n"
        md += "| Algorithm | # Rules | MATILDA Rules | Match % | Completeness % |\n"
        md += "|-----------|---------|---------------|---------|----------------|\n"
        
        for r in dataset_results:
            md += f"| {r['other_algorithm']:10} | {r['other_rules']:7} | "
            md += f"{r['matilda_rules']:13} | {r['match_pct']:6.1f}% | "
            md += f"{r['completeness_pct']:13.1f}% |\n"
        
        md += "\n"
        
        # Metrics comparison (Original metrics as reported by each algorithm)
        md += "### Quality Metrics (Original as reported by each algorithm)\n\n"
        md += "| Algorithm | Avg Original Accuracy | Avg Original Coverage |\n"
        md += "|-----------|----------------------|-----------------------|\n"
        
        for r in dataset_results:
            md += f"| {r['other_algorithm']:10} | "
            md += f"{r['other_original_accuracy']:20.2f} | "
            md += f"{r['other_original_coverage']:21.2f} |\n"
        
        # Add MATILDA's own metrics
        if dataset_results:
            r0 = dataset_results[0]  # Use first result to get MATILDA metrics
            md += f"| MATILDA    | "
            md += f"{r0['matilda_original_accuracy']:20.2f} | "
            md += f"{r0['matilda_original_coverage']:21.2f} |\n"
        
        md += "\n"
        md += "*Note: These are the original accuracy/coverage metrics as reported by each algorithm. "
        md += "MATILDA's validation of competitor rules requires database access and will be implemented in future versions.*\n\n"
        
        # Speed comparison
        md += "### Speed Comparison\n\n"
        md += "| Algorithm | Time (s) | MATILDA Time (s) | Speedup |\n"
        md += "|-----------|----------|------------------|----------|\n"
        
        for r in dataset_results:
            other_time_str = f"{r['other_time']:.2f} Â± {r['other_time_std']:.2f}"
            matilda_time_str = f"{r['matilda_time']:.2f} Â± {r['matilda_time_std']:.2f}"
            
            if r['speedup'] > 1:
                speedup_str = f"**{r['speedup']:.1f}x faster**"
            elif r['speedup'] < 1 and r['speedup'] > 0:
                speedup_str = f"{1/r['speedup']:.1f}x slower"
            else:
                speedup_str = "N/A"
            
            md += f"| {r['other_algorithm']:10} | {other_time_str:15} | "
            md += f"{matilda_time_str:15} | {speedup_str:15} |\n"
        
        md += "\n"
    
    # Summary statistics
    md += "## Overall Summary\n\n"
    
    if results:
        avg_match = statistics.mean([r['match_pct'] for r in results])
        avg_completeness = statistics.mean([r['completeness_pct'] for r in results])
        avg_speedup = statistics.mean([r['speedup'] for r in results if r['speedup'] > 0])
        
        md += f"- **Average Coverage (Match)**: {avg_match:.1f}%\n"
        md += f"- **Average Coverage (Completeness)**: {avg_completeness:.1f}%\n"
        md += f"- **Average Speedup**: {avg_speedup:.1f}x\n\n"
        
        # Interpretation
        md += "### Interpretation\n\n"
        
        if avg_match < 20:
            md += f"- **Low Match ({avg_match:.1f}%)**: MATILDA discovers different rules than baselines, "
            md += "focusing on high-quality, joinable dependencies.\n"
        elif avg_match < 50:
            md += f"- **Moderate Match ({avg_match:.1f}%)**: MATILDA partially aligns with baseline algorithms.\n"
        else:
            md += f"- **High Match ({avg_match:.1f}%)**: MATILDA closely aligns with baseline algorithms.\n"
        
        if avg_completeness < 20:
            md += f"- **Low Completeness ({avg_completeness:.1f}%)**: MATILDA is selective, "
            md += "recovering only the most relevant joinable rules.\n"
        elif avg_completeness < 50:
            md += f"- **Moderate Completeness ({avg_completeness:.1f}%)**: MATILDA recovers a significant "
            md += "portion of joinable baseline rules.\n"
        else:
            md += f"- **High Completeness ({avg_completeness:.1f}%)**: MATILDA recovers most joinable baseline rules.\n"
        
        if avg_speedup > 2:
            md += f"- **Fast ({avg_speedup:.1f}x speedup)**: MATILDA is significantly faster than baselines.\n"
        elif avg_speedup > 1:
            md += f"- **Faster ({avg_speedup:.1f}x speedup)**: MATILDA outperforms baselines in execution time.\n"
        else:
            md += f"- **Comparable Speed**: MATILDA has similar execution time to baselines.\n"
    
    return md


def generate_latex_comparison_table(results: List[Dict]) -> str:
    """Generate LaTeX comparison table."""
    latex = "\\begin{table}[htbp]\n"
    latex += "\\centering\n"
    latex += "\\caption{MATILDA Benchmark: Coverage, Quality Metrics, and Speed Comparison}\n"
    latex += "\\label{tab:matilda_comparison}\n"
    latex += "\\resizebox{\\textwidth}{!}{\n"
    latex += "\\begin{tabular}{llrrrrrrrr}\n"
    latex += "\\toprule\n"
    latex += "\\textbf{Dataset} & \\textbf{Algorithm} & \\textbf{Rules} & "
    latex += "\\textbf{Match \\%} & \\textbf{Compl. \\%} & "
    latex += "\\textbf{Orig. Acc.} & \\textbf{Orig. Cov.} & "
    latex += "\\textbf{Time (s)} & \\textbf{Speedup} \\\\\n"
    latex += "\\midrule\n"
    
    # Group by dataset
    datasets = sorted(set(r['dataset'] for r in results))
    
    for i, dataset in enumerate(datasets):
        dataset_results = [r for r in results if r['dataset'] == dataset]
        
        for j, r in enumerate(dataset_results):
            # First row of dataset includes dataset name
            if j == 0:
                latex += f"{dataset} & "
            else:
                latex += " & "
            
            # Algorithm and metrics
            latex += f"{r['other_algorithm']} & "
            latex += f"{r['other_rules']} / {r['matilda_rules']} & "
            latex += f"{r['match_pct']:.1f} & "
            latex += f"{r['completeness_pct']:.1f} & "
            latex += f"{r['other_original_accuracy']:.2f} & "
            latex += f"{r['other_original_coverage']:.2f} & "
            latex += f"{r['other_time']:.2f} / {r['matilda_time']:.2f} & "
            
            if r['speedup'] > 0:
                latex += f"{r['speedup']:.1f}x"
            else:
                latex += "N/A"
            
            latex += " \\\\\n"
        
        # Add MATILDA row for each dataset
        if dataset_results:
            r0 = dataset_results[0]
            latex += " & MATILDA & "
            latex += f"{r0['matilda_rules']} & "
            latex += "-- & -- & "  # Match/Completeness not applicable for MATILDA
            latex += f"{r0['matilda_original_accuracy']:.2f} & "
            latex += f"{r0['matilda_original_coverage']:.2f} & "
            latex += f"{r0['matilda_time']:.2f} & "
            latex += "1.0x \\\\\n"
        
        # Add separator between datasets (except last one)
        if i < len(datasets) - 1:
            latex += "\\midrule\n"
    
    latex += "\\bottomrule\n"
    latex += "\\end{tabular}\n"
    latex += "}\n"
    latex += "\\end{table}\n"
    
    return latex


def main():
    """Main execution."""
    # Find most recent experiment
    output_dir = Path(__file__).parent / 'data' / 'output'
    mlruns_dir = output_dir / 'mlruns'
    
    if not mlruns_dir.exists():
        print(f"âŒ MLflow directory not found: {mlruns_dir}")
        print("Run benchmark first: python3 run_full_benchmark.py")
        return 1
    
    # Get most recent experiment
    experiments = [d for d in mlruns_dir.iterdir() if d.is_dir()]
    if not experiments:
        print(f"âŒ No experiments found in {mlruns_dir}")
        return 1
    
    # Sort by modification time
    experiments.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    latest_exp = experiments[0]
    
    print(f"ðŸ“Š Analyzing experiment: {latest_exp.name}")
    print(f"ðŸ“‚ Loading data from: {latest_exp}")
    
    # Load experiment data
    experiment = load_mlflow_experiment(latest_exp)
    
    if not experiment['runs']:
        print("âŒ No runs found in experiment")
        return 1
    
    print(f"âœ“ Loaded {len(experiment['runs'])} runs")
    
    # Aggregate runs by algorithm and dataset
    aggregated = aggregate_runs_by_algo_dataset(experiment['runs'])
    print(f"âœ“ Aggregated into {len(aggregated)} algorithm-dataset combinations")
    
    # Generate comparison report
    print(f"\nðŸ”¬ Computing coverage and speed comparisons...")
    results = generate_comparison_report(aggregated, latest_exp)
    
    print(f"\nâœ… Generated {len(results)} comparisons")
    
    # Display summary
    if results:
        print("\n" + "="*60)
        print("SUMMARY: MATILDA vs Baselines")
        print("="*60)
        
        avg_match = statistics.mean([r['match_pct'] for r in results])
        avg_completeness = statistics.mean([r['completeness_pct'] for r in results])
        speedups = [r['speedup'] for r in results if r['speedup'] > 0]
        avg_speedup = statistics.mean(speedups) if speedups else 0
        
        print(f"Average Coverage (Match):        {avg_match:6.1f}%")
        print(f"Average Coverage (Completeness): {avg_completeness:6.1f}%")
        if avg_speedup > 0:
            print(f"Average Speedup:                 {avg_speedup:6.1f}x")
        print("="*60)
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
